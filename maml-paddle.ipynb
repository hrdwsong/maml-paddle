{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 论文复现赛之元学习MAML算法\n",
    "## 1、项目概要\n",
    "### 1.1 文献简介\n",
    "Model-Agnostic Meta-Learning[1]（MAML）算法是一种模型无关的元学习算法，其模型无关体现在，能够与任何使用了梯度下降法的模型相兼容，广泛应用于各种不同的机器学习任务，包括分类、识别、强化学习等领域。\n",
    "\n",
    "元学习的目标，是在大量不同的任务上训练一个模型，使其能够使用极少量的训练数据（即小样本），进行极少量的梯度下降步数，就能够迅速适应新任务，解决新问题。\n",
    "\n",
    "在本项目复现的文献中，通过对模型参数进行显式训练，从而获得在各种任务下均能良好泛化的模型初始化参数。当面临小样本的新任务时，使用该初始化参数，能够在单步（或多步）梯度更新后，实现对该任务的学习和适配。为了复现文献中的实验结果，本项目基于paddlepaddle深度学习框架，在omniglot数据集上进行训练和测试，目标是达到并超过原文献的模型性能。\n",
    "\n",
    "### 1.2 omniglot数据集\n",
    "Omniglot 数据集包含50个不同的字母表，每个字母表中的字母各包含20个手写字符样本，每一个手写样本都是不同的人通过亚马逊的 Mechanical Turk 在线绘制的。Omniglot数据集的多样性强于MNIST数据集，是增强版的MNIST，常用与小样本识别任务。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/cd4e94bac7c3470e800cfb3426fdf8954c5c108ac9504b658be492b034a2fb6b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2、系统方案\n",
    "### 2.1 算法框架\n",
    "考虑一个关于任务T的分布p(T)，我们希望模型能够对该任务分布很好的适配。在K-shot（即K个学习样本）的学习任务下，从p(T)分布中随机采样一个新任务Ti，在任务Ti的样本分布qi中随机采样K个样本，用这K个样本训练模型，获得LOSS，实现对模型f的内循环更新。然后再采样query个样本，评估新模型的LOSS，然后对模型f进行外循环更新。反复上述过程，从而使最终模型能够对任务分布p(T)上的所有情况，能够良好地泛化。算法可用下图进行示意。\n",
    "\n",
    " ![](https://ai-studio-static-online.cdn.bcebos.com/5c1cc7e52f7e4a3d98b9693a6e27309c72d041b310994d889640a29221e47c52)\n",
    " \n",
    "2.2 算法流程\n",
    "MAML算法针对小样本图像分类任务的计算流程，如下图所示：\n",
    " \n",
    " ![](https://ai-studio-static-online.cdn.bcebos.com/bd44f95ed7564189a010b04367f79ba15362cbf1dc9c491ea539ffb2b06dfa23)\n",
    " \n",
    "本项目的难点在于，算法包含外循环和内循环两种梯度更新方式。内循环针对每一种任务T进行梯度更新，用更新后的模型重新评估LOSS；而外循环则要使用内循环中更新后的LOSS，在所有任务上更新原始模型。\n",
    "使用paddle经典的动态图框架，在内循环更新完成后，模型各节点参数已经发生变化，loss已无法反传到先前的模型参数上。外循环的参数更新公式为\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/2bf80b14ecee42d88b19eceae07ce5fa4a7d15f1cf764ce89ddee5b719270f51)\n",
    "\n",
    "\n",
    "这里，要使用θ_i^'参数模型计算的LOSS，反传回θ，使用经典动态图模型架构无法实现。本方案通过自定义参数的方式，使函数层层级联，实现更灵活的参数控制。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3、系统代码和数据\n",
    "### 3.1 数据准备与预处理（只需执行一遍）\n",
    "\n",
    "本项目使用AI Studio上的“omniglot元学习数据集”，大小为64.6M。\n",
    "\n",
    "首先解压数据集，并将images_background和images_evaluation路径下的内容，拷贝到“data/omniglot/”中。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!unzip -oq /home/aistudio/data/data78550/omniglot_python.zip -d /home/aistudio/data/omniglot_pre\n",
    "!cp -r /home/aistudio/data/omniglot_pre/images_background/. /home/aistudio/data/omniglot\n",
    "!cp -r /home/aistudio/data/omniglot_pre/images_evaluation/. /home/aistudio/data/omniglot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "对图像数据进行遍历、处理，构建训练集、验证集和测试集的numpy格式数据，并保存到工程根目录下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of character folders: 1623\n",
      "The number of train characters is 973\n",
      "The number of validation characters is 325\n",
      "The number of test characters is 325\n",
      "The shape of train_imgs: (973, 20, 1, 28, 28)\n",
      "The shape of val_imgs: (325, 20, 1, 28, 28)\n",
      "The shape of test_imgs: (325, 20, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "import os\r\n",
    "import cv2\r\n",
    "import numpy as np\r\n",
    "import random\r\n",
    "\r\n",
    "data_folder = './data/omniglot'  # omniglot数据集路径\r\n",
    "\r\n",
    "character_folders = [os.path.join(data_folder, family, character) \\\r\n",
    "                     for family in os.listdir(data_folder) \\\r\n",
    "                     if os.path.isdir(os.path.join(data_folder, family)) \\\r\n",
    "                     for character in os.listdir(os.path.join(data_folder, family))]\r\n",
    "print(\"The number of character folders: {}\".format(len(character_folders)))  # 1623\r\n",
    "random.seed(1)\r\n",
    "random.shuffle(character_folders)\r\n",
    "train_folders = character_folders[:973]\r\n",
    "val_folders = character_folders[973:1298]\r\n",
    "test_folders = character_folders[1298:]\r\n",
    "print('The number of train characters is {}'.format(len(train_folders)))  # 973\r\n",
    "print('The number of validation characters is {}'.format(len(val_folders)))  # 325\r\n",
    "print('The number of test characters is {}'.format(len(test_folders)))  # 325\r\n",
    "\r\n",
    "train_imgs_list = []\r\n",
    "for char_fold in train_folders:\r\n",
    "    char_list = []\r\n",
    "    for file in [os.path.join(char_fold, f) for f in os.listdir(char_fold)]:\r\n",
    "        img = cv2.imread(file)\r\n",
    "        img = cv2.resize(img, (28, 28))\r\n",
    "        img = np.transpose(img, (2, 0, 1))\r\n",
    "        img = img[0].astype('float32')  # 只取零通道\r\n",
    "        img = img / 255.0\r\n",
    "        img = img * 2.0 - 1.0\r\n",
    "        char_list.append(img)\r\n",
    "    char_list = np.array(char_list)\r\n",
    "    train_imgs_list.append(char_list)\r\n",
    "train_imgs = np.array(train_imgs_list)\r\n",
    "train_imgs = train_imgs[:, :, np.newaxis, :, :]\r\n",
    "print('The shape of train_imgs: {}'.format(train_imgs.shape))  # [973,20,1,28,28]\r\n",
    "\r\n",
    "val_imgs_list = []\r\n",
    "for char_fold in val_folders:\r\n",
    "    char_list = []\r\n",
    "    for file in [os.path.join(char_fold, f) for f in os.listdir(char_fold)]:\r\n",
    "        img = cv2.imread(file)\r\n",
    "        img = cv2.resize(img, (28, 28))\r\n",
    "        img = np.transpose(img, (2, 0, 1))\r\n",
    "        img = img[0].astype('float32')  # 只取零通道\r\n",
    "        img = img / 255.0\r\n",
    "        img = img * 2.0 - 1.0\r\n",
    "        char_list.append(img)\r\n",
    "    char_list = np.array(char_list)\r\n",
    "    val_imgs_list.append(char_list)\r\n",
    "val_imgs = np.array(val_imgs_list)\r\n",
    "val_imgs = val_imgs[:, :, np.newaxis, :, :]\r\n",
    "print('The shape of val_imgs: {}'.format(val_imgs.shape))  # [325,20,1,28,28]\r\n",
    "\r\n",
    "test_imgs_list = []\r\n",
    "for char_fold in test_folders:\r\n",
    "    char_list = []\r\n",
    "    for file in [os.path.join(char_fold, f) for f in os.listdir(char_fold)]:\r\n",
    "        img = cv2.imread(file)\r\n",
    "        img = cv2.resize(img, (28, 28))\r\n",
    "        img = np.transpose(img, (2, 0, 1))\r\n",
    "        img = img[0].astype('float32')  # 只取零通道\r\n",
    "        img = img / 255.0\r\n",
    "        img = img * 2.0 - 1.0\r\n",
    "        char_list.append(img)\r\n",
    "    char_list = np.array(char_list)\r\n",
    "    test_imgs_list.append(char_list)\r\n",
    "test_imgs = np.array(test_imgs_list)\r\n",
    "test_imgs = test_imgs[:, :, np.newaxis, :, :]\r\n",
    "print('The shape of test_imgs: {}'.format(test_imgs.shape))  # [325,20,1,28,28]\r\n",
    "\r\n",
    "np.save(os.path.join('omniglot_train.npy'), train_imgs)\r\n",
    "np.save(os.path.join('omniglot_val.npy'), val_imgs)\r\n",
    "np.save(os.path.join('omniglot_test.npy'), test_imgs)\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "打开并显示四个样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "# 加载训练集和测试集\r\n",
    "x_train = np.load('omniglot_train.npy')  # (964, 20, 1, 28, 28)\r\n",
    "plt.subplot(1,4,1)\r\n",
    "plt.imshow(x_train[0,0,0,:,:], cmap=plt.cm.gray)\r\n",
    "plt.axis('off')\r\n",
    "plt.subplot(1,4,2)\r\n",
    "plt.imshow(x_train[1,0,0,:,:], cmap=plt.cm.gray)\r\n",
    "plt.axis('off')\r\n",
    "plt.subplot(1,4,3)\r\n",
    "plt.imshow(x_train[2,0,0,:,:], cmap=plt.cm.gray)\r\n",
    "plt.axis('off')\r\n",
    "plt.subplot(1,4,4)\r\n",
    "plt.imshow(x_train[3,0,0,:,:], cmap=plt.cm.gray)\r\n",
    "plt.axis('off')\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.2 训练脚本及日志\n",
    "#### 3.3.1 导入库，并设置全局训练参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  def convert_to_list(value, n, name, dtype=np.int):\n"
     ]
    }
   ],
   "source": [
    "import paddle\r\n",
    "import paddle.nn as nn\r\n",
    "import paddle.nn.functional as F\r\n",
    "import numpy as np\r\n",
    "import random\r\n",
    "import time\r\n",
    "from copy import deepcopy, copy\r\n",
    "from tqdm import tqdm\r\n",
    "import pickle\r\n",
    "\r\n",
    "# 加载训练集和测试集\r\n",
    "x_train = np.load('omniglot_train.npy')  # (973, 20, 1, 28, 28)\r\n",
    "x_val = np.load('omniglot_val.npy')  # (325, 20, 1, 28, 28)\r\n",
    "x_test = np.load('omniglot_test.npy')  # (325, 20, 1, 28, 28)\r\n",
    "datasets = {'train': x_train, 'val': x_val, 'test': x_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "5-way-1-shot修改配置如下：n_way = 5, k_spt = 1\n",
    "\n",
    "5-way-5-shot修改配置如下：n_way = 5, k_spt = 5\n",
    "\n",
    "20-way-1-shot修改配置如下：n_way = 20, k_spt = 1\n",
    "\n",
    "20-way-5-shot修改配置如下：n_way = 20, k_spt = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB: train (973, 20, 1, 28, 28) validation (325, 20, 1, 28, 28) test (325, 20, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# 全局参数设置\r\n",
    "n_way = 5\r\n",
    "k_spt = 1  # support data 的个数\r\n",
    "k_query = 15  # query data 的个数\r\n",
    "imgsz = 28\r\n",
    "resize = imgsz\r\n",
    "task_num = 32\r\n",
    "batch_size = task_num\r\n",
    "glob_update_step = 5\r\n",
    "glob_update_step_test = 5\r\n",
    "glob_meta_lr = 0.001  # 外循环学习率\r\n",
    "glob_base_lr = 0.1  # 内循环学习率\r\n",
    "\r\n",
    "indexes = {\"train\": 0, \"val\": 0, \"test\": 0}\r\n",
    "print(\"DB: train\", x_train.shape, \"validation\", x_val.shape, \"test\", x_test.shape)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "定义函数load_data_cache，用于加载若干组batch数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data_cache(dataset):\r\n",
    "    \"\"\"\r\n",
    "    Collects several batches data for N-shot learning\r\n",
    "    :param dataset: [cls_num, 20, 84, 84, 1]\r\n",
    "    :return: A list with [support_set_x, support_set_y, target_x, target_y] ready to be fed to our networks\r\n",
    "    \"\"\"\r\n",
    "    #  take 5 way 1 shot as example: 5 * 1\r\n",
    "    setsz = k_spt * n_way\r\n",
    "    querysz = k_query * n_way\r\n",
    "    data_cache = []\r\n",
    "\r\n",
    "    # print('preload next 10 caches of batch_size of batch.')\r\n",
    "    for sample in range(50):  # num of epochs\r\n",
    "\r\n",
    "        x_spts, y_spts, x_qrys, y_qrys = [], [], [], []\r\n",
    "        for i in range(batch_size):  # one batch means one set\r\n",
    "\r\n",
    "            x_spt, y_spt, x_qry, y_qry = [], [], [], []\r\n",
    "            selected_cls = np.random.choice(dataset.shape[0], n_way, replace=False)\r\n",
    "\r\n",
    "            for j, cur_class in enumerate(selected_cls):\r\n",
    "                selected_img = np.random.choice(20, k_spt + k_query, replace=False)\r\n",
    "\r\n",
    "                # 构造support集和query集\r\n",
    "                x_spt.append(dataset[cur_class][selected_img[:k_spt]])\r\n",
    "                x_qry.append(dataset[cur_class][selected_img[k_spt:]])\r\n",
    "                y_spt.append([j for _ in range(k_spt)])\r\n",
    "                y_qry.append([j for _ in range(k_query)])\r\n",
    "\r\n",
    "            # shuffle inside a batch\r\n",
    "            perm = np.random.permutation(n_way * k_spt)\r\n",
    "            x_spt = np.array(x_spt).reshape(n_way * k_spt, 1, resize, resize)[perm]\r\n",
    "            y_spt = np.array(y_spt).reshape(n_way * k_spt)[perm]\r\n",
    "            perm = np.random.permutation(n_way * k_query)\r\n",
    "            x_qry = np.array(x_qry).reshape(n_way * k_query, 1, resize, resize)[perm]\r\n",
    "            y_qry = np.array(y_qry).reshape(n_way * k_query)[perm]\r\n",
    "\r\n",
    "            # append [sptsz, 1, 84, 84] => [batch_size, setsz, 1, 84, 84]\r\n",
    "            x_spts.append(x_spt)\r\n",
    "            y_spts.append(y_spt)\r\n",
    "            x_qrys.append(x_qry)\r\n",
    "            y_qrys.append(y_qry)\r\n",
    "\r\n",
    "        #         print(x_spts[0].shape)\r\n",
    "        # [b, setsz = n_way * k_spt, 1, 28, 28]\r\n",
    "        x_spts = np.array(x_spts).astype(np.float32).reshape(batch_size, setsz, 1, resize, resize)\r\n",
    "        y_spts = np.array(y_spts).astype(np.int64).reshape(batch_size, setsz)\r\n",
    "        # [b, qrysz = n_way * k_query, 1, 28, 28]\r\n",
    "        x_qrys = np.array(x_qrys).astype(np.float32).reshape(batch_size, querysz, 1, resize, resize)\r\n",
    "        y_qrys = np.array(y_qrys).astype(np.int64).reshape(batch_size, querysz)\r\n",
    "        #         print(x_qrys.shape)\r\n",
    "        data_cache.append([x_spts, y_spts, x_qrys, y_qrys])\r\n",
    "\r\n",
    "    return data_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "获取训练和测试时用到的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets_cache = {\"train\": load_data_cache(x_train),  # current epoch data cached\r\n",
    "                  \"val\": load_data_cache(x_val),\r\n",
    "                  \"test\": load_data_cache(x_test)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "获取一个batch的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def next(mode='train'):\r\n",
    "    \"\"\"\r\n",
    "    Gets next batch from the dataset with name.\r\n",
    "    :param mode: The name of the splitting (one of \"train\", \"val\", \"test\")\r\n",
    "    :return:\r\n",
    "    \"\"\"\r\n",
    "    # 如果所需的index超出当前已经获取的数量，则重新执行load_data_cache获取新的数据\r\n",
    "    if indexes[mode] >= len(datasets_cache[mode]):\r\n",
    "        indexes[mode] = 0\r\n",
    "        datasets_cache[mode] = load_data_cache(datasets[mode])\r\n",
    "\r\n",
    "    next_batch = datasets_cache[mode][indexes[mode]]\r\n",
    "    indexes[mode] += 1\r\n",
    "\r\n",
    "    return next_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.3.2 定义网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MAML(paddle.nn.Layer):\r\n",
    "    def __init__(self):\r\n",
    "        super(MAML, self).__init__()\r\n",
    "        # 定义模型中全部待优化参数\r\n",
    "        self.vars = []\r\n",
    "        self.vars_bn = []\r\n",
    "\r\n",
    "        # ------------------------第1个conv2d-------------------------\r\n",
    "        weight = paddle.static.create_parameter(shape=[64, 1, 3, 3],\r\n",
    "                                                dtype='float32',\r\n",
    "                                                default_initializer=nn.initializer.KaimingNormal(),  # 参数可以修改为Xavier\r\n",
    "                                                is_bias=False)\r\n",
    "        bias = paddle.static.create_parameter(shape=[64],\r\n",
    "                                              dtype='float32',\r\n",
    "                                              is_bias=True)  # 初始化为零\r\n",
    "        self.vars.extend([weight, bias])\r\n",
    "        # 第1个BatchNorm\r\n",
    "        weight = paddle.static.create_parameter(shape=[64],\r\n",
    "                                                dtype='float32',\r\n",
    "                                                default_initializer=nn.initializer.Constant(value=1),  # 参数可以修改为Xavier\r\n",
    "                                                is_bias=False)\r\n",
    "        bias = paddle.static.create_parameter(shape=[64],\r\n",
    "                                              dtype='float32',\r\n",
    "                                              is_bias=True)  # 初始化为零\r\n",
    "        self.vars.extend([weight, bias])\r\n",
    "        running_mean = paddle.to_tensor(np.zeros([64], np.float32), stop_gradient=True)\r\n",
    "        running_var = paddle.to_tensor(np.zeros([64], np.float32), stop_gradient=True)\r\n",
    "        self.vars_bn.extend([running_mean, running_var])\r\n",
    "\r\n",
    "        # ------------------------第2个conv2d------------------------\r\n",
    "        weight = paddle.static.create_parameter(shape=[64, 64, 3, 3],\r\n",
    "                                                dtype='float32',\r\n",
    "                                                default_initializer=nn.initializer.KaimingNormal(),  # 参数可以修改为Xavier\r\n",
    "                                                is_bias=False)\r\n",
    "        bias = paddle.static.create_parameter(shape=[64],\r\n",
    "                                              dtype='float32',\r\n",
    "                                              is_bias=True)\r\n",
    "        self.vars.extend([weight, bias])\r\n",
    "        # 第2个BatchNorm\r\n",
    "        weight = paddle.static.create_parameter(shape=[64],\r\n",
    "                                                dtype='float32',\r\n",
    "                                                default_initializer=nn.initializer.Constant(value=1),  # 参数可以修改为Xavier\r\n",
    "                                                is_bias=False)\r\n",
    "        bias = paddle.static.create_parameter(shape=[64],\r\n",
    "                                              dtype='float32',\r\n",
    "                                              is_bias=True)  # 初始化为零\r\n",
    "        self.vars.extend([weight, bias])\r\n",
    "        running_mean = paddle.to_tensor(np.zeros([64], np.float32), stop_gradient=True)\r\n",
    "        running_var = paddle.to_tensor(np.zeros([64], np.float32), stop_gradient=True)\r\n",
    "        self.vars_bn.extend([running_mean, running_var])\r\n",
    "\r\n",
    "        # ------------------------第3个conv2d------------------------\r\n",
    "        weight = paddle.static.create_parameter(shape=[64, 64, 3, 3],\r\n",
    "                                                dtype='float32',\r\n",
    "                                                default_initializer=nn.initializer.KaimingNormal(),  # 参数可以修改为Xavier\r\n",
    "                                                is_bias=False)\r\n",
    "        bias = paddle.static.create_parameter(shape=[64],\r\n",
    "                                              dtype='float32',\r\n",
    "                                              is_bias=True)\r\n",
    "        self.vars.extend([weight, bias])\r\n",
    "        # 第3个BatchNorm\r\n",
    "        weight = paddle.static.create_parameter(shape=[64],\r\n",
    "                                                dtype='float32',\r\n",
    "                                                default_initializer=nn.initializer.Constant(value=1),  # 参数可以修改为Xavier\r\n",
    "                                                is_bias=False)\r\n",
    "        bias = paddle.static.create_parameter(shape=[64],\r\n",
    "                                              dtype='float32',\r\n",
    "                                              is_bias=True)  # 初始化为零\r\n",
    "        self.vars.extend([weight, bias])\r\n",
    "        running_mean = paddle.to_tensor(np.zeros([64], np.float32), stop_gradient=True)\r\n",
    "        running_var = paddle.to_tensor(np.zeros([64], np.float32), stop_gradient=True)\r\n",
    "        self.vars_bn.extend([running_mean, running_var])\r\n",
    "\r\n",
    "        # ------------------------第4个conv2d------------------------\r\n",
    "        weight = paddle.static.create_parameter(shape=[64, 64, 3, 3],\r\n",
    "                                                dtype='float32',\r\n",
    "                                                default_initializer=nn.initializer.KaimingNormal(),  # 参数可以修改为Xavier\r\n",
    "                                                is_bias=False)\r\n",
    "        bias = paddle.static.create_parameter(shape=[64],\r\n",
    "                                              dtype='float32',\r\n",
    "                                              is_bias=True)\r\n",
    "        self.vars.extend([weight, bias])\r\n",
    "        # 第4个BatchNorm\r\n",
    "        weight = paddle.static.create_parameter(shape=[64],\r\n",
    "                                                dtype='float32',\r\n",
    "                                                default_initializer=nn.initializer.Constant(value=1),  # 参数可以修改为Xavier\r\n",
    "                                                is_bias=False)\r\n",
    "        bias = paddle.static.create_parameter(shape=[64],\r\n",
    "                                              dtype='float32',\r\n",
    "                                              is_bias=True)  # 初始化为零\r\n",
    "        self.vars.extend([weight, bias])\r\n",
    "        running_mean = paddle.to_tensor(np.zeros([64], np.float32), stop_gradient=True)\r\n",
    "        running_var = paddle.to_tensor(np.zeros([64], np.float32), stop_gradient=True)\r\n",
    "        self.vars_bn.extend([running_mean, running_var])\r\n",
    "\r\n",
    "        # ------------------------全连接层------------------------\r\n",
    "        weight = paddle.static.create_parameter(shape=[64, n_way],\r\n",
    "                                                dtype='float32',\r\n",
    "                                                default_initializer=nn.initializer.XavierNormal(),\r\n",
    "                                                is_bias=False)\r\n",
    "        bias = paddle.static.create_parameter(shape=[n_way],\r\n",
    "                                              dtype='float32',\r\n",
    "                                              is_bias=True)\r\n",
    "        self.vars.extend([weight, bias])\r\n",
    "\r\n",
    "    def forward(self, x, params=None, bn_training=True):\r\n",
    "        \"\"\"\r\n",
    "        :param x: 输入图片\r\n",
    "        :param params:\r\n",
    "        :param bn_training: set False to not update\r\n",
    "        :return: 输出分类\r\n",
    "        \"\"\"\r\n",
    "        if params is None:\r\n",
    "            params = self.vars\r\n",
    "\r\n",
    "        weight, bias = params[0], params[1]  # 第1个CONV层\r\n",
    "        x = F.conv2d(x, weight, bias, stride=1, padding=1)\r\n",
    "        weight, bias = params[2], params[3]  # 第1个BN层\r\n",
    "        running_mean, running_var = self.vars_bn[0], self.vars_bn[1]\r\n",
    "        x = F.batch_norm(x, running_mean, running_var, weight=weight, bias=bias, training=bn_training)\r\n",
    "        x = F.relu(x)  # 第1个relu\r\n",
    "        x = F.max_pool2d(x, kernel_size=2)  # 第1个MAX_POOL层\r\n",
    "\r\n",
    "        weight, bias = params[4], params[5]  # 第2个CONV层\r\n",
    "        x = F.conv2d(x, weight, bias, stride=1, padding=1)\r\n",
    "        weight, bias = params[6], params[7]  # 第2个BN层\r\n",
    "        running_mean, running_var = self.vars_bn[2], self.vars_bn[3]\r\n",
    "        x = F.batch_norm(x, running_mean, running_var, weight=weight, bias=bias, training=bn_training)\r\n",
    "        x = F.relu(x)  # 第2个relu\r\n",
    "        x = F.max_pool2d(x, kernel_size=2)  # 第2个MAX_POOL层\r\n",
    "\r\n",
    "        weight, bias = params[8], params[9]  # 第3个CONV层\r\n",
    "        x = F.conv2d(x, weight, bias, stride=1, padding=1)\r\n",
    "        weight, bias = params[10], params[11]  # 第3个BN层\r\n",
    "        running_mean, running_var = self.vars_bn[4], self.vars_bn[5]\r\n",
    "        x = F.batch_norm(x, running_mean, running_var, weight=weight, bias=bias, training=bn_training)\r\n",
    "        x = F.relu(x)  # 第3个relu\r\n",
    "        x = F.max_pool2d(x, kernel_size=2)  # 第3个MAX_POOL层\r\n",
    "\r\n",
    "        weight, bias = params[12], params[13]  # 第4个CONV层\r\n",
    "        x = F.conv2d(x, weight, bias, stride=1, padding=1)\r\n",
    "        weight, bias = params[14], params[15]  # 第4个BN层\r\n",
    "        running_mean, running_var = self.vars_bn[6], self.vars_bn[7]\r\n",
    "        x = F.batch_norm(x, running_mean, running_var, weight=weight, bias=bias, training=bn_training)\r\n",
    "        x = F.relu(x)  # 第4个relu\r\n",
    "        x = F.max_pool2d(x, kernel_size=2)  # 第4个MAX_POOL层\r\n",
    "\r\n",
    "        x = paddle.reshape(x, [x.shape[0], -1])  ## flatten\r\n",
    "        weight, bias = params[-2], params[-1]  # linear\r\n",
    "        x = F.linear(x, weight, bias)\r\n",
    "\r\n",
    "        output = x\r\n",
    "\r\n",
    "        return output\r\n",
    "\r\n",
    "    def parameters(self, include_sublayers=True):\r\n",
    "        return self.vars\r\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "验证网络输入输出维度是否正确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 5]\n"
     ]
    }
   ],
   "source": [
    "# 输入[32,1,28,28]，输出[32,5]\n",
    "model = MAML()\n",
    "\n",
    "x = np.random.randn(*[32, 1, 28, 28]).astype('float32')\n",
    "x = paddle.to_tensor(x)\n",
    "y = model(x)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.3.4 定义元学习类\n",
    "该类包括了训练和测试时的学习行为。以下两个参数用于定义内循环和外循环的学习率\n",
    "\n",
    "self.meta_lr = 0.001  # 外循环学习率\n",
    "\n",
    "self.base_lr = 0.1  # 内循环学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MetaLearner(nn.Layer):\r\n",
    "    def __init__(self):\r\n",
    "        super(MetaLearner, self).__init__()\r\n",
    "        self.update_step = glob_update_step  # task-level inner update steps\r\n",
    "        self.update_step_test = glob_update_step_test\r\n",
    "        self.net = MAML()\r\n",
    "        self.meta_lr = glob_meta_lr  # 外循环学习率\r\n",
    "        self.base_lr = glob_base_lr  # 内循环学习率\r\n",
    "        self.meta_optim = paddle.optimizer.Adam(learning_rate=self.meta_lr, parameters=self.net.parameters())\r\n",
    "        # self.meta_optim = paddle.optimizer.Momentum(learning_rate=self.meta_lr,\r\n",
    "        #                                             parameters=self.net.parameters(),\r\n",
    "        #                                             momentum=0.9)\r\n",
    "\r\n",
    "    def forward(self, x_spt, y_spt, x_qry, y_qry):\r\n",
    "        task_num = x_spt.shape[0]\r\n",
    "        query_size = x_qry.shape[1]  # 75 = 15 * 5\r\n",
    "        loss_list_qry = [0 for _ in range(self.update_step + 1)]\r\n",
    "        correct_list = [0 for _ in range(self.update_step + 1)]\r\n",
    "\r\n",
    "        # 内循环梯度手动更新，外循环梯度使用定义好的更新器更新\r\n",
    "        for i in range(task_num):\r\n",
    "            # 第0步更新\r\n",
    "            y_hat = self.net(x_spt[i], params=None, bn_training=True)  # (setsz, ways)\r\n",
    "            loss = F.cross_entropy(y_hat, y_spt[i])\r\n",
    "            grad = paddle.grad(loss, self.net.parameters())  # 计算所有loss相对于参数的梯度和\r\n",
    "\r\n",
    "            tuples = zip(grad, self.net.parameters())  # 将梯度和参数一一对应起来\r\n",
    "            # fast_weights这一步相当于求了一个\\theta - \\alpha*\\nabla(L)\r\n",
    "            fast_weights = list(map(lambda p: p[1] - self.base_lr * p[0], tuples))\r\n",
    "            # 在query集上测试，计算准确率\r\n",
    "            # 这一步使用更新前的数据，loss填入loss_list_qry[0]，预测正确数填入correct_list[0]\r\n",
    "            with paddle.no_grad():\r\n",
    "                y_hat = self.net(x_qry[i], self.net.parameters(), bn_training=True)\r\n",
    "                loss_qry = F.cross_entropy(y_hat, y_qry[i])\r\n",
    "                loss_list_qry[0] += loss_qry\r\n",
    "                pred_qry = F.softmax(y_hat, axis=1).argmax(axis=1)  # size = (75)  # axis取-1也行\r\n",
    "                correct = paddle.equal(pred_qry, y_qry[i]).numpy().sum().item()\r\n",
    "                correct_list[0] += correct\r\n",
    "                # 使用更新后的数据在query集上测试。loss填入loss_list_qry[1]，预测正确数填入correct_list[1]\r\n",
    "            with paddle.no_grad():\r\n",
    "                y_hat = self.net(x_qry[i], fast_weights, bn_training=True)\r\n",
    "                loss_qry = F.cross_entropy(y_hat, y_qry[i])\r\n",
    "                loss_list_qry[1] += loss_qry\r\n",
    "                pred_qry = F.softmax(y_hat, axis=1).argmax(axis=1)  # size = (75)\r\n",
    "                correct = paddle.equal(pred_qry, y_qry[i]).numpy().sum().item()\r\n",
    "                correct_list[1] += correct\r\n",
    "\r\n",
    "            # 剩余更新步数\r\n",
    "            for k in range(1, self.update_step):\r\n",
    "                y_hat = self.net(x_spt[i], params=fast_weights, bn_training=True)\r\n",
    "                loss = F.cross_entropy(y_hat, y_spt[i])\r\n",
    "                grad = paddle.grad(loss, fast_weights)\r\n",
    "                tuples = zip(grad, fast_weights)\r\n",
    "                fast_weights = list(map(lambda p: p[1] - self.base_lr * p[0], tuples))\r\n",
    "\r\n",
    "                if k < self.update_step - 1:\r\n",
    "                    with paddle.no_grad():\r\n",
    "                        y_hat = self.net(x_qry[i], params=fast_weights, bn_training=True)\r\n",
    "                        loss_qry = F.cross_entropy(y_hat, y_qry[i])\r\n",
    "                        loss_list_qry[k + 1] += loss_qry\r\n",
    "                else:  # 对于最后一步update，要记录loss计算的梯度值，便于外循环的梯度传播\r\n",
    "                    y_hat = self.net(x_qry[i], params=fast_weights, bn_training=True)\r\n",
    "                    loss_qry = F.cross_entropy(y_hat, y_qry[i])\r\n",
    "                    loss_list_qry[k + 1] += loss_qry\r\n",
    "\r\n",
    "                with paddle.no_grad():\r\n",
    "                    pred_qry = F.softmax(y_hat, axis=1).argmax(axis=1)\r\n",
    "                    correct = paddle.equal(pred_qry, y_qry[i]).numpy().sum().item()\r\n",
    "                    correct_list[k + 1] += correct\r\n",
    "\r\n",
    "        loss_qry = loss_list_qry[-1] / task_num  # 计算最后一次loss的平均值\r\n",
    "        self.meta_optim.clear_grad()  # 梯度清零\r\n",
    "        loss_qry.backward()\r\n",
    "        self.meta_optim.step()\r\n",
    "\r\n",
    "        accs = np.array(correct_list) / (query_size * task_num)  # 计算各更新步数acc的平均值\r\n",
    "        loss = np.array(loss_list_qry) / task_num  # 计算各更新步数loss的平均值\r\n",
    "        return accs, loss\r\n",
    "\r\n",
    "    def finetunning(self, x_spt, y_spt, x_qry, y_qry):\r\n",
    "        # assert len(x_spt.shape) == 4\r\n",
    "\r\n",
    "        query_size = x_qry.shape[0]\r\n",
    "        correct_list = [0 for _ in range(self.update_step_test + 1)]\r\n",
    "\r\n",
    "        new_net = deepcopy(self.net)\r\n",
    "        y_hat = new_net(x_spt)\r\n",
    "        loss = F.cross_entropy(y_hat, y_spt)\r\n",
    "        grad = paddle.grad(loss, new_net.parameters())\r\n",
    "        fast_weights = list(map(lambda p: p[1] - self.base_lr * p[0], zip(grad, new_net.parameters())))\r\n",
    "\r\n",
    "        # 在query集上测试，计算准确率\r\n",
    "        # 这一步使用更新前的数据\r\n",
    "        with paddle.no_grad():\r\n",
    "            y_hat = new_net(x_qry, params=new_net.parameters(), bn_training=True)\r\n",
    "            pred_qry = F.softmax(y_hat, axis=1).argmax(axis=1)  # size = (75)\r\n",
    "            correct = paddle.equal(pred_qry, y_qry).numpy().sum().item()\r\n",
    "            correct_list[0] += correct\r\n",
    "\r\n",
    "        # 使用更新后的数据在query集上测试。\r\n",
    "        with paddle.no_grad():\r\n",
    "            y_hat = new_net(x_qry, params=fast_weights, bn_training=True)\r\n",
    "            pred_qry = F.softmax(y_hat, axis=1).argmax(axis=1)  # size = (75)\r\n",
    "            correct = paddle.equal(pred_qry, y_qry).numpy().sum().item()\r\n",
    "            correct_list[1] += correct\r\n",
    "\r\n",
    "        for k in range(1, self.update_step_test):\r\n",
    "            y_hat = new_net(x_spt, params=fast_weights, bn_training=True)\r\n",
    "            loss = F.cross_entropy(y_hat, y_spt)\r\n",
    "            grad = paddle.grad(loss, fast_weights)\r\n",
    "            fast_weights = list(map(lambda p: p[1] - self.base_lr * p[0], zip(grad, fast_weights)))\r\n",
    "\r\n",
    "            y_hat = new_net(x_qry, fast_weights, bn_training=True)\r\n",
    "\r\n",
    "            with paddle.no_grad():\r\n",
    "                pred_qry = F.softmax(y_hat, axis=1).argmax(axis=1)\r\n",
    "                correct = paddle.equal(pred_qry, y_qry).numpy().sum().item()\r\n",
    "                correct_list[k + 1] += correct\r\n",
    "\r\n",
    "        del new_net\r\n",
    "        accs = np.array(correct_list) / query_size\r\n",
    "        return accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.3.5 启动训练和测试过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------5-way-5-shot task start!---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:77: VisibleDeprecationWarning: Creating an ndarray from nested sequences exceeding the maximum number of dimensions of 32 is deprecated. If you mean to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "[0.20208333 0.50875    0.59541667 0.72875    0.83333333 0.88583333]\n",
      "---------------------在992个随机任务上测试：---------------------\n",
      "验证集准确率: [0.2025 0.483  0.5723 0.674  0.7896 0.8535]\n",
      "------------------------------------------------------------\n",
      "epoch: 100\n",
      "[0.21083333 0.80416667 0.90416667 0.94083333 0.945      0.9475    ]\n",
      "epoch: 200\n",
      "[0.2        0.94291667 0.9625     0.96625    0.97375    0.97625   ]\n",
      "epoch: 300\n",
      "[0.2        0.9625     0.97375    0.97625    0.97666667 0.97666667]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------执行训练----------------------------------------\r\n",
    "# omniglot\r\n",
    "# 设置随机数种子\r\n",
    "random.seed(1337)\r\n",
    "np.random.seed(1337)\r\n",
    "\r\n",
    "# 开启0号GPU训练\r\n",
    "use_gpu = True\r\n",
    "paddle.set_device('gpu:0') if use_gpu else paddle.set_device('cpu')\r\n",
    "\r\n",
    "meta = MetaLearner()\r\n",
    "best_acc = 0\r\n",
    "epochs = 10000\r\n",
    "print('--------------------{}-way-{}-shot task start!---------------------'.format(n_way, k_spt))\r\n",
    "# for step in tqdm(range(epochs)):\r\n",
    "for step in range(epochs):\r\n",
    "    # start = time.time()\r\n",
    "    x_spt, y_spt, x_qry, y_qry = next('train')\r\n",
    "    x_spt = paddle.to_tensor(x_spt)\r\n",
    "    y_spt = paddle.to_tensor(y_spt)\r\n",
    "    x_qry = paddle.to_tensor(x_qry)\r\n",
    "    y_qry = paddle.to_tensor(y_qry)\r\n",
    "    accs, loss = meta(x_spt, y_spt, x_qry, y_qry)\r\n",
    "    # end = time.time()\r\n",
    "    if step % 100 == 0:\r\n",
    "        print(\"epoch:\", step)\r\n",
    "        print(accs)\r\n",
    "    #         print(loss)\r\n",
    "\r\n",
    "    if step % 1000 == 0:\r\n",
    "        accs = []\r\n",
    "        for _ in range(1000 // task_num):\r\n",
    "            x_spt, y_spt, x_qry, y_qry = next('val')\r\n",
    "            x_spt = paddle.to_tensor(x_spt)\r\n",
    "            y_spt = paddle.to_tensor(y_spt)\r\n",
    "            x_qry = paddle.to_tensor(x_qry)\r\n",
    "            y_qry = paddle.to_tensor(y_qry)\r\n",
    "\r\n",
    "            for x_spt_one, y_spt_one, x_qry_one, y_qry_one in zip(x_spt, y_spt, x_qry, y_qry):\r\n",
    "                test_acc = meta.finetunning(x_spt_one, y_spt_one, x_qry_one, y_qry_one)\r\n",
    "                accs.append(test_acc)\r\n",
    "\r\n",
    "        print('---------------------在{}个随机任务上测试：---------------------'.format(np.array(accs).shape[0]))\r\n",
    "        accs = np.array(accs).mean(axis=0).astype(np.float16)\r\n",
    "        print('验证集准确率:', accs)\r\n",
    "        print('------------------------------------------------------------')\r\n",
    "        # 记录并保存最佳模型\r\n",
    "        if accs[-1] > best_acc:\r\n",
    "            best_acc = accs[-1]\r\n",
    "            model_params = [item.numpy() for item in meta.net.vars]\r\n",
    "            model_params_file = open('model_param_best_%sway%sshot.pkl' % (n_way, k_spt), 'wb')\r\n",
    "            pickle.dump(model_params, model_params_file)\r\n",
    "            model_params_file.close()\r\n",
    "print('The best acc on validation set is {}'.format(best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.3 测试脚本及日志\n",
    "可以在work/best_module/路径下找到已经训练好的最优模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------加载模型----------------------------------------\r\n",
    "model_params_file = open('model_param_best_%sway%sshot.pkl' % (n_way, k_spt), 'rb')\r\n",
    "model_params = pickle.load(model_params_file)\r\n",
    "model_params_file.close()\r\n",
    "meta = MetaLearner()\r\n",
    "meta.net.vars = [paddle.to_tensor(item, stop_gradient=False) for item in model_params]\r\n",
    "\r\n",
    "# ------------------------------------------执行测试----------------------------------------\r\n",
    "accs = []\r\n",
    "for _ in range(1000 // task_num):\r\n",
    "    # db_train.next('test')\r\n",
    "    x_spt, y_spt, x_qry, y_qry = next('test')\r\n",
    "    x_spt = paddle.to_tensor(x_spt)\r\n",
    "    y_spt = paddle.to_tensor(y_spt)\r\n",
    "    x_qry = paddle.to_tensor(x_qry)\r\n",
    "    y_qry = paddle.to_tensor(y_qry)\r\n",
    "\r\n",
    "    for x_spt_one, y_spt_one, x_qry_one, y_qry_one in zip(x_spt, y_spt, x_qry, y_qry):\r\n",
    "        test_acc = meta.finetunning(x_spt_one, y_spt_one, x_qry_one, y_qry_one)\r\n",
    "        accs.append(test_acc)\r\n",
    "\r\n",
    "print('---------------------在{}个随机任务上测试：---------------------'.format(np.array(accs).shape[0]))\r\n",
    "accs = np.array(accs).mean(axis=0).astype(np.float16)\r\n",
    "print('测试集准确率:', accs)\r\n",
    "print('------------------------------------------------------------')\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.4 最终精度和模型优化\n",
    "#### 3.4.1 精度和最佳超参配置\n",
    "基于paddlepaddle深度学习框架，对文献MAML进行复现后，汇总各小样本任务下的测试精度，如下表所示。\n",
    "|任务|Test ACC|range|文献值|\n",
    "|----|----|----|----|\n",
    "|5-way-1-shot|99.2%|98.3%|98.7%|\n",
    "|5-way-5-shot|99.5%|99.8%|99.9%|\n",
    "|20-way-1-shot|95.0%|95.5%|95.8%|\n",
    "|20-way-5-shot|98.7%|98.7%|98.9%|\n",
    "\n",
    "超参数配置如下表所示：\n",
    "|超参数名|设置值|\n",
    "|----|----|\n",
    "|batch_size|32|\n",
    "|update_step|5|\n",
    "|update_step_test|5|\n",
    "|meta_lr|0.001|\n",
    "|base_lr|0.1|\n",
    "\n",
    "#### 3.4.2 关于最优模型保存\n",
    "由于MAML算法的特殊性，便于模型参数在内外两层循环间进行梯度反向传播，本项目网络架构是基于paddle.nn.Layer进行自定义的方式实现的，模型不存在state_dict类型的参数，无法通过调用paddle.save函数保存模型。因此，首先将模型参数从Parameter类型转换为numpy数组，用pickle进行打包保存。加载时先用pickle加载为numpy对象，在赋值到模型参数中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4、结论和展望\n",
    "本项目基于paddlepaddle深度学习框架，对MAML元学习算法代码进行改写，并复现文献中的实验数据。基于paddle.nn.Layer对模型进行自定义设计，实现了MAML所要求的内外循环间反向梯度传播。在此基础上，完成了文献中关于omniglot数据集上小样本识别问题的研究和实验复现，得到以下结论：\n",
    "\n",
    "1、在各种任务下，识别精度逼近原文献数值，在5-way-1-shot下精度超过原文献数值。\n",
    "\n",
    "2、paddlepaddle深度学习框架是一种高效、简洁、易用、灵活的深度学习框架，为广大科研及工程设计人员提供了便捷的深度学习设计接口；本项目基于该框架，完成了论文复现赛的要求，达到了较高的指标和良好的实验效果。\n",
    "\n",
    "由于MAML内外双循环的特性，导致两种学习率的设置、调试难度系数很大，单次实验所需时间很长，在比赛限制的时间周期内，难以获得最优的超参数值。下一步，在时间和算力允许的条件下，可以进一步优化超参数，提高实验精度。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5、参考文献\n",
    "[1] Finn C., Abbeel P., Levine S. Model-agnostic meta-learning for fast adaptation of deep networks[C]. International Conference on Machine Learning, PMLR, 1126-1135."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
